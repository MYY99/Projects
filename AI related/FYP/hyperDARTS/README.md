# Differentiable Neural Architecture Search through Hypernetworks
Neural Architecture Search (NAS) is a technique used in the field of artificial intelligence and deep learning to automatically search for optimal neural network architectures. Traditionally, designing effective neural networks required significant human expertise and trial-and-error experimentation. However, NAS automates this process by using machine learning algorithms to search through a predefined search space of possible architectures. It involves evaluating and comparing various architectures based on their performance on a given task, such as image classification or natural language processing. NAS algorithms utilize strategies like reinforcement learning, evolutionary algorithms, or gradient-based optimization to efficiently explore the vast space of potential architectures and identify those that exhibit superior performance. By automating the architecture design process, NAS holds the promise of accelerating the development of state-of-the-art neural networks and driving advancements in artificial intelligence research and applications.

DARTS (Differentiable Architecture Search) is a gradient-based approach to (NAS). It uses a continuous relaxation of the architecture space, allowing for efficient optimization through backpropagation. By introducing differentiable variables and leveraging the softmax relaxation, DARTS enables joint optimization of architecture and model weights. This approach has proven effective in discovering competitive neural architectures while reducing computational costs. DARTS uses unrolling method to approach the bilevel optimization search problem to approximate hypergradient. This project proposes developing a DARTS variant by replacing the unrolling method with hypernetworks to eliminate the approximation loss. 

![image](https://github.com/MYY99/Projects/assets/133868293/acc277d1-dca8-4ede-a386-bbad35c4c581)


